"""
ELYZA-japanese-Llama-2-7b ãƒ¢ãƒ‡ãƒ«ã®é«˜é€Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
hf_transferã‚’ä½¿ç”¨ã—ã¦é«˜é€Ÿã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""
import os
from huggingface_hub import snapshot_download

# hf_transferã«ã‚ˆã‚‹é«˜é€Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

def download_elyza_model():
    """ELYZA-japanese-Llama-2-7bã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆsafetensorsç‰ˆï¼‰"""
    model_name = "elyza/ELYZA-japanese-Llama-2-7b"
    
    print(f"ğŸ“¥ Downloading {model_name} (safetensors format)...")
    print("âš¡ Using hf_transfer for accelerated download")
    print("=" * 60)
    
    try:
        local_dir = snapshot_download(
            repo_id=model_name,
            local_dir=f"./models/{model_name.split('/')[-1]}-safetensors",
            local_dir_use_symlinks=False,
            resume_download=True,
            ignore_patterns=["*.bin", "*.pth", "pytorch_model*"]  # pytorchãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
        )
        
        print("=" * 60)
        print(f"âœ… Model downloaded successfully!")
        print(f"ğŸ“ Location: {local_dir}")
        print("\nğŸ”§ Usage in code:")
        print(f'    model = AutoModelForCausalLM.from_pretrained("{local_dir}")')
        
        return local_dir
        
    except Exception as e:
        print(f"âŒ Download failed: {e}")
        return None

if __name__ == "__main__":
    download_elyza_model()

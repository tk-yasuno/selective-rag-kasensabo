"""
ColBERT RAG Implementation for Kasensabo
æ²³å·ç ‚é˜²ãƒ€ãƒ æŠ€è¡“åŸºæº–ç”¨ColBERTæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 

ç‰¹å¾´:
- ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ•°å€¤ãƒ»ç”¨èªã«å¼·ã„ï¼‰
- é…å»¶ç›¸äº’ä½œç”¨ï¼ˆLate Interactionï¼‰
- æ©‹æ¢è¨ºæ–­ã§+10-15%ã®å®Ÿç¸¾
"""

import torch
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict
import logging
from dataclasses import dataclass
import pickle
import json

from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModel

from config import *

logging.basicConfig(level=LOG_LEVEL, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ColBERTDocument:
    """ColBERTç”¨ã®æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    content: str
    doc_id: int
    metadata: Dict
    embeddings: torch.Tensor = None  # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿


class ColBERTRAG:
    """
    ColBERT-based Retrieval Augmented Generation
    
    ColBERTã®é…å»¶ç›¸äº’ä½œç”¨ã‚’æ´»ç”¨ã—ãŸæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã€‚
    å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—å…¨ä½“ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—ã€‚
    """
    
    def __init__(self, model_name: str = COLBERT_MODEL):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: ColBERTãƒ¢ãƒ‡ãƒ«åï¼ˆHuggingFaceï¼‰
        """
        logger.info(f"Initializing ColBERT RAG with {model_name}")
        
        self.device = DEVICE
        self.model_name = model_name
        
        # ColBERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
        logger.info("Loading ColBERT model and tokenizer...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name).to(self.device)
            self.model.eval()
            # Use fp16 on GPU to reduce memory if available
            if self.device == 'cuda':
                try:
                    self.model.half()
                    logger.info('Using model.half() for reduced memory (fp16)')
                except Exception:
                    logger.debug('model.half() not supported for this model')
            logger.info("âœ“ ColBERT model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load ColBERT model: {e}")
            logger.info("Falling back to BERT-base model for demonstration")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¨™æº–BERTãƒ¢ãƒ‡ãƒ«
            self.tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
            self.model = AutoModel.from_pretrained("bert-base-multilingual-cased").to(self.device)
            self.model.eval()
        
        self.documents: List[ColBERTDocument] = []
        self.index_built = False
        
    def load_documents(self, data_dir: Path = DATA_DIR) -> int:
        """
        æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            èª­ã¿è¾¼ã‚“ã ãƒãƒ£ãƒ³ã‚¯æ•°
        """
        logger.info(f"Loading documents from {data_dir}")
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«èª­ã¿è¾¼ã¿
        from langchain_community.document_loaders import TextLoader
        
        raw_docs = []
        for filepath in Path(data_dir).glob("**/*.md"):
            try:
                loader = TextLoader(str(filepath), encoding='utf-8')
                raw_docs.extend(loader.load())
            except Exception as e:
                logger.warning(f"Failed to load {filepath}: {e}")
        
        logger.info(f"Loaded {len(raw_docs)} markdown files")
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
        )
        
        chunks = splitter.split_documents(raw_docs)
        logger.info(f"Split into {len(chunks)} chunks")
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨
        if SAMPLE_RATIO < 1.0:
            import random
            sample_size = int(len(chunks) * SAMPLE_RATIO)
            chunks = random.sample(chunks, sample_size)
            logger.info(f"ğŸ“Š Sampled {len(chunks)} chunks ({SAMPLE_RATIO*100:.0f}% of total)")
        
        # ColBERTDocumentå½¢å¼ã«å¤‰æ›
        for idx, chunk in enumerate(chunks):
            doc = ColBERTDocument(
                content=chunk.page_content[:MAX_DOCUMENT_LENGTH * 4],  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
                doc_id=idx,
                metadata=chunk.metadata
            )
            self.documents.append(doc)
        
        logger.info(f"Loaded {len(self.documents)} documents")
        return len(self.documents)
    
    def encode_document(self, text: str) -> torch.Tensor:
        """
        æ–‡æ›¸ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã§åŸ‹ã‚è¾¼ã¿
        
        Args:
            text: æ–‡æ›¸ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (seq_len, hidden_dim)
        """
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=MAX_DOCUMENT_LENGTH,
            padding="max_length"
        ).to(self.device)
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model(**inputs)
            # æœ€çµ‚å±¤ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
            embeddings = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden_dim)
        
        return embeddings

    def encode_documents_batch(self, texts: List[str], batch_size: int = 8) -> List[torch.Tensor]:
        """
        è¤‡æ•°æ–‡æ›¸ã‚’ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã‚’è¿”ã™ã€‚
        ã™ã¹ã¦ã®å‡ºåŠ›ã¯CPUã«ç§»ã—ã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’åˆ†æ•£ã™ã‚‹ã€‚
        """
        results: List[torch.Tensor] = []
        self.model.eval()
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    truncation=True,
                    max_length=MAX_DOCUMENT_LENGTH,
                    padding='max_length',
                    return_tensors='pt'
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                outputs = self.model(**inputs)
                emb = outputs.last_hidden_state  # (batch, seq_len, hidden_dim)
                # move to cpu and append per-document tensor
                emb_cpu = emb.cpu()
                for b in range(emb_cpu.size(0)):
                    results.append(emb_cpu[b])
                # clear GPU cache between batches
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        return results
    
    def encode_query(self, query: str) -> torch.Tensor:
        """
        ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã§åŸ‹ã‚è¾¼ã¿
        
        Args:
            query: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ³ã‚½ãƒ« (seq_len, hidden_dim)
        """
        return self.encode_document(query)  # åŒã˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    
    def build_index(self):
        """
        å…¨æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: åŸ‹ã‚è¾¼ã¿ã‚’CPUã«ä¿å­˜ã€å®šæœŸçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        """
        if not self.documents:
            raise ValueError("No documents loaded. Call load_documents() first.")
        
        logger.info(f"Building ColBERT index for {len(self.documents)} documents (batched)...")

        texts = [d.content for d in self.documents]
        batch_size = 8
        # adaptively increase batch size if GPU memory available
        if self.device == 'cuda':
            batch_size = 16

        encoded_list = self.encode_documents_batch(texts, batch_size=batch_size)

        # Assign embeddings back to documents (already on CPU)
        for idx, emb in enumerate(encoded_list):
            if idx % 200 == 0:
                logger.info(f"  Assigned embeddings for doc {idx}/{len(self.documents)}")
            self.documents[idx].embeddings = emb

        # final cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.index_built = True
        logger.info("âœ“ ColBERT index built successfully (batched)")
    
    def compute_colbert_score(
        self,
        query_embeddings: torch.Tensor,
        doc_embeddings: torch.Tensor
    ) -> float:
        """
        ColBERTã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆMaxSimé…å»¶ç›¸äº’ä½œç”¨ï¼‰- GPUé«˜é€ŸåŒ–
        
        å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ã€æœ€ã‚‚é¡ä¼¼ã™ã‚‹æ–‡æ›¸ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’åˆè¨ˆ
        
        Args:
            query_embeddings: ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ (q_len, hidden_dim)
            doc_embeddings: æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ (d_len, hidden_dim)
            
        Returns:
            ColBERTã‚¹ã‚³ã‚¢
        """
        # GPUã«ç§»å‹•ã—ã¦è¨ˆç®—
        device = self.device if hasattr(self, 'device') else 'cpu'
        query_gpu = query_embeddings.to(device)
        doc_gpu = doc_embeddings.to(device)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®— (q_len, d_len)
        query_norm = torch.nn.functional.normalize(query_gpu, p=2, dim=1)
        doc_norm = torch.nn.functional.normalize(doc_gpu, p=2, dim=1)
        
        similarity_matrix = torch.matmul(query_norm, doc_norm.T)  # (q_len, d_len)
        
        # å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€å¤§é¡ä¼¼åº¦ã‚’åˆè¨ˆï¼ˆMaxSimï¼‰
        max_similarities = similarity_matrix.max(dim=1)[0]  # (q_len,)
        
        # ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§æ­£è¦åŒ–ï¼ˆ0-1ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«ï¼‰
        num_query_tokens = query_gpu.size(0)
        colbert_score = max_similarities.sum().item() / num_query_tokens
        
        return colbert_score
    
    def search(self, query: str, top_k: int = TOP_K_RETRIEVAL) -> List[Tuple[str, float, Dict]]:
        """
        ColBERTæ¤œç´¢ï¼ˆé«˜é€ŸåŒ–: top_k*10å€™è£œã«çµã£ã¦ã‹ã‚‰è©³ç´°ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: è¿”ã™çµæœæ•°
            
        Returns:
            [(content, score, metadata), ...]
        """
        if not self.index_built:
            raise ValueError("Index not built. Call build_index() first.")
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
        query_embeddings = self.encode_query(query)
        
        # é«˜é€ŸåŒ–: å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã§å€™è£œã‚’çµã‚‹ï¼ˆGPUä½¿ç”¨ï¼‰
        query_mean = query_embeddings.mean(dim=0, keepdim=True).to(self.device)  # (1, hidden_dim)
        
        candidate_count = min(top_k * 10, len(self.documents))  # å€™è£œã‚’10å€ã«çµã‚‹
        
        # å…¨æ–‡æ›¸ã®å¹³å‡åŸ‹ã‚è¾¼ã¿ã‚’ãƒãƒƒãƒè¨ˆç®—
        doc_means = torch.stack([doc.embeddings.mean(dim=0) for doc in self.documents]).to(self.device)
        
        # ãƒãƒƒãƒã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        simple_scores = torch.nn.functional.cosine_similarity(
            query_mean, doc_means, dim=1
        )
        
        # ä¸Šä½å€™è£œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        top_indices = torch.topk(simple_scores, k=candidate_count).indices.cpu().numpy()
        
        # è©³ç´°ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆColBERT MaxSimï¼‰- GPUã§é«˜é€ŸåŒ–
        scores = []
        for idx in top_indices:
            doc = self.documents[idx]
            score = self.compute_colbert_score(query_embeddings, doc.embeddings)
            scores.append((doc, score))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # Top-kå–å¾—
        results = []
        for doc, score in scores[:top_k]:
            results.append((
                doc.content,
                score,
                {**doc.metadata, 'doc_id': doc.doc_id}
            ))
        
        return results
    
    def save(self, filepath: Path):
        """
        ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        
        Args:
            filepath: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        logger.info(f"Saving ColBERT index to {filepath}")
        
        save_data = {
            'documents': self.documents,
            'model_name': self.model_name,
            'index_built': self.index_built
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        logger.info("âœ“ Index saved")
    
    def load(self, filepath: Path):
        """
        ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            filepath: èª­ã¿è¾¼ã¿å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        logger.info(f"Loading ColBERT index from {filepath}")
        
        with open(filepath, 'rb') as f:
            save_data = pickle.load(f)
        
        self.documents = save_data['documents']
        self.index_built = save_data['index_built']
        
        logger.info(f"âœ“ Loaded {len(self.documents)} documents")


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("="*80)
    print("ColBERT RAG for Kasensabo")
    print("æ²³å·ç ‚é˜²ãƒ€ãƒ ColBERTæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*80)
    
    # åˆæœŸåŒ–
    colbert = ColBERTRAG()
    
    # æ–‡æ›¸èª­ã¿è¾¼ã¿
    num_docs = colbert.load_documents()
    print(f"\nâœ“ Loaded {num_docs} documents")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    colbert.build_index()
    print("âœ“ Index built")
    
    # ä¿å­˜
    save_path = OUTPUT_DIR / "colbert_rag.pkl"
    colbert.save(save_path)
    print(f"âœ“ Saved to {save_path}")
    
    # ãƒ†ã‚¹ãƒˆæ¤œç´¢
    test_queries = [
        "ç ‚é˜²ãƒ€ãƒ ã®ç‚¹æ¤œã§é‡è¦–ã™ã¹ãé …ç›®ã¯",
        "å ¤é˜²ã®è¨­è¨ˆåŸºæº–å€¤ã¯",
        "æµé‡è¦³æ¸¬ã®æ‰‹æ³•"
    ]
    
    print("\n" + "="*80)
    print("Test Search")
    print("="*80)
    
    for query in test_queries:
        print(f"\nQuery: {query}")
        results = colbert.search(query, top_k=3)
        
        for i, (content, score, metadata) in enumerate(results, 1):
            print(f"{i}. Score={score:.2f}, Source={metadata.get('source', 'N/A')}")
            print(f"   {content[:100]}...")


if __name__ == "__main__":
    main()

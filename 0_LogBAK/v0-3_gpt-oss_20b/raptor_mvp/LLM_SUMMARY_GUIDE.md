# æ²³å·ç ‚é˜²RAPTOR - è¦ç´„LLMã®ç¾çŠ¶ã¨æ¨å¥¨

## ğŸ“Š ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³

### ç¾åœ¨ã®MVPï¼ˆraptor_mvp/raptor_rag.pyï¼‰
**è¦ç´„æ–¹æ³•**: **LLMã‚’ä½¿ç”¨ã—ã¦ã„ãªã„ï¼ˆå˜ç´”ãªæ–‡å­—åˆ—çµåˆï¼‰**

```python
def _create_summary(self, node_ids: List[str]) -> str:
    """ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰è¦ç´„ã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼šå…ˆé ­éƒ¨åˆ†ã‚’çµåˆï¼‰"""
    texts = [self.nodes[nid].content for nid in node_ids[:5]]  # æœ€å¤§5ä»¶
    combined = "\n\n".join([t[:200] for t in texts])
    
    if len(combined) > 800:
        combined = combined[:800] + "..."
    
    return combined
```

**å•é¡Œç‚¹**:
- âŒ å®Ÿéš›ã®è¦ç´„ãŒç”Ÿæˆã•ã‚Œãªã„ï¼ˆå˜ãªã‚‹æ–‡å­—åˆ—åˆ‡ã‚Šå–ã‚Šï¼‰
- âŒ å†—é•·ãªæƒ…å ±ãŒå«ã¾ã‚Œã‚‹
- âŒ æ–‡è„ˆã®ç†è§£ãŒãªã„

---

## ğŸš€ ãƒ™ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ï¼ˆtrue_raptor_builder.pyï¼‰ã®å®Ÿè£…

### ä½¿ç”¨ã—ã¦ã„ã‚‹LLMï¼ˆ16GB GPUå¯¾å¿œï¼‰

```python
# GPUå®¹é‡ã«å¿œã˜ãŸè‡ªå‹•é¸æŠ
if gpu_memory >= 24:  # 24GBä»¥ä¸Š
    llm_model_name = "facebook/opt-6.7b"  # 6.7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
elif gpu_memory >= 16:  # 16GBä»¥ä¸Š â† ã‚ãªãŸã®ã‚±ãƒ¼ã‚¹
    llm_model_name = "facebook/opt-2.7b"  # 2.7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
elif gpu_memory >= 12:
    llm_model_name = "facebook/opt-1.3b"
elif gpu_memory >= 8:
    llm_model_name = "microsoft/DialoGPT-large"
else:
    llm_model_name = "microsoft/DialoGPT-medium"
```

**16GB GPUã®å ´åˆ**: `facebook/opt-2.7b` ã‚’ä½¿ç”¨

### è¦ç´„ç”Ÿæˆã®å®Ÿè£…

```python
def generate_llm_summary(self, documents: List[str]) -> str:
    """GPUå¯¾å¿œã®å¤§è¦æ¨¡LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆ"""
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆæ²³å·ç ‚é˜²ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
    prompt = f"""Summarize the following findings in a concise scientific manner.
Focus on key mechanisms and processes.

Findings: {combined_text}

Summary:"""
    
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = self.llm_tokenizer.encode(
        prompt, 
        return_tensors="pt", 
        truncation=True, 
        max_length=800,
        padding=True
    )
    
    # ç”Ÿæˆï¼ˆGPUæœ€é©åŒ–ï¼‰
    with torch.no_grad():
        outputs = self.llm_model.generate(
            inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1
        )
    
    summary = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary[:400]
```

---

## ğŸ¯ æ²³å·ç ‚é˜²ãƒ€ãƒ æŠ€è¡“åŸºæº–ã«æœ€é©ãªLLMé¸æŠè‚¢

### 1ï¸âƒ£ **æ¨å¥¨: æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆ16GB GPUå¯¾å¿œï¼‰**

#### **elyza/ELYZA-japanese-Llama-2-7b** â­ æœ€æ¨å¥¨
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 7B
- **ç‰¹å¾´**: æ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸLlama 2
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„14GBï¼ˆ16GBã«åã¾ã‚‹ï¼‰
- **åˆ©ç‚¹**: 
  - âœ… æ—¥æœ¬èªã®æŠ€è¡“æ–‡æ›¸ã«æœ€é©
  - âœ… æ²³å·ç ‚é˜²ã®å°‚é–€ç”¨èªã‚’æ­£ã—ãç†è§£
  - âœ… 16GBã§å¿«é©ã«å‹•ä½œ

```python
llm_model_name = "elyza/ELYZA-japanese-Llama-2-7b"
self.llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)
```

#### **rinna/japanese-gpt-neox-3.6b**
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 3.6B
- **ç‰¹å¾´**: æ—¥æœ¬èªGPT-NeoX
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„7GB
- **åˆ©ç‚¹**:
  - âœ… ã‚ˆã‚Šè»½é‡ã§é«˜é€Ÿ
  - âœ… ãƒ¡ãƒ¢ãƒªã«ä½™è£•

#### **cyberagent/open-calm-7b**
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 7B
- **ç‰¹å¾´**: æ—¥æœ¬èªç‰¹åŒ–ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„14GB

---

### 2ï¸âƒ£ **å¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ï¼ˆè‹±èªæ–‡æ›¸ã‚‚å«ã‚€å ´åˆï¼‰**

#### **meta-llama/Llama-2-7b-chat-hf** â­ ãƒãƒ©ãƒ³ã‚¹å‹
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 7B
- **ç‰¹å¾´**: Metaå…¬å¼ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„14GB
- **åˆ©ç‚¹**:
  - âœ… é«˜å“è³ªãªè¦ç´„ç”Ÿæˆ
  - âœ… æŒ‡ç¤ºè¿½å¾“æ€§ãŒé«˜ã„
  - âœ… è‹±èªãƒ»æ—¥æœ¬èªä¸¡å¯¾å¿œï¼ˆè³ªã¯ã‚„ã‚„ä½ä¸‹ï¼‰

#### **mistralai/Mistral-7B-Instruct-v0.2** â­ é«˜æ€§èƒ½
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 7B
- **ç‰¹å¾´**: æœ€æ–°ã®åŠ¹ç‡çš„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„14GB
- **åˆ©ç‚¹**:
  - âœ… åŒã‚µã‚¤ã‚ºã§æœ€é«˜ã‚¯ãƒ©ã‚¹ã®æ€§èƒ½
  - âœ… é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¼·ã„

---

### 3ï¸âƒ£ **è»½é‡ãƒ»é«˜é€Ÿãƒ¢ãƒ‡ãƒ«ï¼ˆä½™è£•ã‚’æŒãŸã›ãŸã„å ´åˆï¼‰**

#### **stabilityai/japanese-stablelm-instruct-alpha-7b**
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 7B
- **ç‰¹å¾´**: æ—¥æœ¬èªStableLM
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„14GB

#### **facebook/opt-2.7b**ï¼ˆç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ï¼‰
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: 2.7B
- **ç‰¹å¾´**: Meta OPTã‚·ãƒªãƒ¼ã‚º
- **ãƒ¡ãƒ¢ãƒª**: FP16ã§ç´„5.4GB
- **åˆ©ç‚¹**:
  - âœ… éå¸¸ã«è»½é‡
  - âŒ æ—¥æœ¬èªå¯¾å¿œãŒå¼±ã„

---

## ğŸ”§ å®Ÿè£…ä¾‹ï¼šæ²³å·ç ‚é˜²ç”¨ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸè¦ç´„ç”Ÿæˆ

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class KasensaboRAPTORWithLLM:
    def __init__(self):
        # æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        model_name = "elyza/ELYZA-japanese-Llama-2-7b"
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.llm_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            device_map="auto",          # GPUè‡ªå‹•é…ç½®
            low_cpu_mem_usage=True
        )
        self.llm_model.eval()
    
    def _create_summary(self, node_ids: List[str]) -> str:
        """LLMã‚’ä½¿ã£ã¦è¦ç´„ç”Ÿæˆ"""
        texts = [self.nodes[nid].content for nid in node_ids[:5]]
        combined = "\n\n".join([t[:300] for t in texts])
        
        # æ²³å·ç ‚é˜²å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®æ²³å·ç ‚é˜²ãƒ€ãƒ æŠ€è¡“åŸºæº–ã®æ–‡æ›¸ã‚’ã€å°‚é–€çš„ã‹ã¤ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
é‡è¦ãªæŠ€è¡“ç”¨èªã€åŸºæº–å€¤ã€è¨­è¨ˆæ‰‹æ³•ã‚’ä¿æŒã—ã¦ãã ã•ã„ã€‚

æ–‡æ›¸:
{combined}

è¦ç´„:"""
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to("cuda")
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.llm_model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.5,  # æŠ€è¡“æ–‡æ›¸ãªã®ã§ä½ã‚ã«è¨­å®š
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.2,
                no_repeat_ngram_size=3
            )
        
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
        if "è¦ç´„:" in summary:
            summary = summary.split("è¦ç´„:")[-1].strip()
        
        return summary[:500]
```

---

## ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨ï¼ˆ16GB GPUï¼‰

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º | ãƒ¡ãƒ¢ãƒª(FP16) | æ—¥æœ¬èª | é€Ÿåº¦ | å“è³ª | æ¨å¥¨åº¦ |
|--------|--------|--------------|--------|------|------|--------|
| **ELYZA-japanese-Llama-2-7b** | 7B | 14GB | â­â­â­ | â­â­ | â­â­â­ | **ğŸ¥‡** |
| **Mistral-7B-Instruct** | 7B | 14GB | â­â­ | â­â­â­ | â­â­â­ | **ğŸ¥ˆ** |
| **rinna/japanese-gpt-neox-3.6b** | 3.6B | 7GB | â­â­â­ | â­â­â­ | â­â­ | **ğŸ¥‰** |
| facebook/opt-2.7b (ç¾çŠ¶) | 2.7B | 5.4GB | â­ | â­â­â­ | â­ | - |

---

## âš¡ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### 4bité‡å­åŒ–ï¼ˆã•ã‚‰ã«å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ï¼‰

```python
from transformers import BitsAndBytesConfig

# 4bité‡å­åŒ–è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# 13Bãƒ¢ãƒ‡ãƒ«ã‚‚16GBã§å‹•ä½œå¯èƒ½
model = AutoModelForCausalLM.from_pretrained(
    "elyza/ELYZA-japanese-Llama-2-13b",  # 13Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    quantization_config=bnb_config,
    device_map="auto"
)
```

**çµæœ**: 13Bãƒ¢ãƒ‡ãƒ«ãŒç´„10GBã§å‹•ä½œ

---

## ğŸ¯ æœ€çµ‚æ¨å¥¨

### **æ²³å·ç ‚é˜²ãƒ€ãƒ æŠ€è¡“åŸºæº–ã«æœ€é©**: `elyza/ELYZA-japanese-Llama-2-7b`

**ç†ç”±**:
1. âœ… æ—¥æœ¬èªæŠ€è¡“æ–‡æ›¸ã®ç†è§£ãŒå„ªç§€
2. âœ… 16GBã§å¿«é©ã«å‹•ä½œ
3. âœ… å°‚é–€ç”¨èªã‚’æ­£ã—ãä¿æŒ
4. âœ… è¦ç´„ã®å“è³ªãŒé«˜ã„

### å®Ÿè£…æ‰‹é †

1. **ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
```bash
pip install transformers accelerate bitsandbytes
```

2. **raptor_rag.pyã«çµ±åˆ**
```python
# config.pyã«è¿½åŠ 
LLM_MODEL = "elyza/ELYZA-japanese-Llama-2-7b"
USE_LLM_SUMMARY = True

# raptor_rag.pyã§åˆæœŸåŒ–
if USE_LLM_SUMMARY:
    self._init_llm()
```

3. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**
```bash
python main.py build
```

ã“ã‚Œã§é«˜å“è³ªãªæ—¥æœ¬èªè¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼

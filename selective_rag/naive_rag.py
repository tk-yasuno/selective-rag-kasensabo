"""
Naive RAG Module - ã‚·ãƒ³ãƒ—ãƒ«ãªFAISS + Sentence-Transformerså®Ÿè£…
"""

import logging
from typing import List, Tuple, Dict
import numpy as np
from pathlib import Path

logger = logging.getLogger(__name__)


class NaiveRAG:
    """
    ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³RAGå®Ÿè£…
    FAISS + Sentence-Transformers (all-MiniLM-L6-v2)
    """
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Args:
            model_name: Sentence-Transformersãƒ¢ãƒ‡ãƒ«å
        """
        from sentence_transformers import SentenceTransformer
        import faiss
        
        logger.info(f"Initializing Naive RAG with {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        self.documents = []
        self.index = None
        self.model_name = model_name
        
    def load_documents(
        self,
        data_dir: Path,
        chunk_size: int = 500,
        chunk_overlap: int = 100,
        sample_ratio: float = 1.0
    ):
        """
        æ–‡æ›¸èª­ã¿è¾¼ã¿ã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            sample_ratio: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡ï¼ˆ0.0-1.0ï¼‰
        """
        from langchain_community.document_loaders import TextLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        logger.info(f"Loading documents from {data_dir}")
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        raw_docs = []
        for filepath in data_dir.glob("**/*.md"):
            try:
                loader = TextLoader(str(filepath), encoding='utf-8')
                raw_docs.extend(loader.load())
            except Exception as e:
                logger.warning(f"Failed to load {filepath}: {e}")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_documents(raw_docs)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if sample_ratio < 1.0:
            import random
            sample_size = int(len(chunks) * sample_ratio)
            chunks = random.sample(chunks, sample_size)
            logger.info(f"ğŸ“Š Sampled {len(chunks)} chunks ({sample_ratio*100:.0f}%)")
        
        self.documents = [(chunk.page_content, chunk.metadata) for chunk in chunks]
        logger.info(f"âœ“ Loaded {len(self.documents)} documents")
        
    def build_index(self):
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        import faiss
        
        logger.info("Building FAISS index...")
        
        texts = [doc[0] for doc in self.documents]
        embeddings = self.embedding_model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        
        # Inner Productç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ­£è¦åŒ–æ¸ˆã¿â†’ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨ç­‰ä¾¡ï¼‰
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings.astype('float32'))
        
        logger.info(f"âœ“ Built FAISS index with {self.index.ntotal} documents")
        
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """
        ã‚¯ã‚¨ãƒªæ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ä»¶æ•°
        
        Returns:
            [(content, score, metadata), ...] ã®ãƒªã‚¹ãƒˆ
        """
        if self.index is None:
            raise ValueError("Index not built. Call build_index() first.")
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # æ¤œç´¢
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                content, metadata = self.documents[idx]
                results.append((content, float(score), metadata))
        
        return results
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "system_name": "Naive RAG",
            "model": self.model_name,
            "total_documents": len(self.documents),
            "index_size": self.index.ntotal if self.index else 0,
        }


def test_naive_rag():
    """Naive RAGã®ãƒ†ã‚¹ãƒˆ"""
    from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP
    
    print("\n=== Naive RAG Test ===")
    
    # åˆæœŸåŒ–
    rag = NaiveRAG()
    
    # æ–‡æ›¸èª­ã¿è¾¼ã¿
    rag.load_documents(
        data_dir=DATA_DIR,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        sample_ratio=0.1  # ãƒ†ã‚¹ãƒˆç”¨ã«10%
    )
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    rag.build_index()
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "å ¤é˜²ã®å¤©ç«¯å¹…ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ",
        "æ²³å·ç®¡ç†ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    ]
    
    for query in test_queries:
        print(f"\nQuery: {query}")
        start = time.time()
        results = rag.search(query, top_k=3)
        elapsed = (time.time() - start) * 1000
        
        print(f"Time: {elapsed:.2f}ms")
        for i, (content, score, metadata) in enumerate(results, 1):
            print(f"  {i}. Score: {score:.4f}")
            print(f"     {content[:100]}...")
    
    # çµ±è¨ˆ
    stats = rag.get_stats()
    print(f"\n{stats}")


if __name__ == "__main__":
    import time
    test_naive_rag()

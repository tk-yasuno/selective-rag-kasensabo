"""
ColBERT RAG Module - Token-level late interaction retrieval
"""

import torch
import numpy as np
import logging
from pathlib import Path
from typing import List, Tuple, Dict
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ColBERTDocument:
    """ColBERTç”¨ã®æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    content: str
    doc_id: int
    metadata: Dict
    embeddings: torch.Tensor = None  # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®åŸ‹ã‚è¾¼ã¿


class ColBERTRAG:
    """
    ColBERT-based RAG with Token-level Late Interaction
    
    ç‰¹å¾´:
    - ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ•°å€¤ãƒ»å›ºæœ‰åè©ã«å¼·ã„ï¼‰
    - MaxSimé…å»¶ç›¸äº’ä½œç”¨
    - 2æ®µéšæ¤œç´¢ï¼ˆmean pooling filter + MaxSim rankingï¼‰
    """
    
    def __init__(self, model_name: str = "colbert-ir/colbertv2.0"):
        """
        Args:
            model_name: ColBERTãƒ¢ãƒ‡ãƒ«å
        """
        from transformers import AutoTokenizer, AutoModel
        
        logger.info(f"Initializing ColBERT RAG with {model_name}")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = model_name
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        logger.info("Loading ColBERT model...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        # fp16æœ€é©åŒ–ï¼ˆGPUï¼‰
        if self.device == 'cuda':
            self.model.half()
            logger.info('Using fp16 for reduced memory')
        
        self.documents: List[ColBERTDocument] = []
        self.index_built = False
        
        logger.info(f"âœ“ ColBERT RAG initialized on {self.device}")
    
    def load_documents(
        self,
        data_dir: Path,
        chunk_size: int = 500,
        chunk_overlap: int = 100,
        sample_ratio: float = 0.5,
        max_length: int = 512
    ):
        """
        æ–‡æ›¸èª­ã¿è¾¼ã¿ã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            sample_ratio: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡
            max_length: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·
        """
        from langchain_community.document_loaders import TextLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        logger.info(f"Loading documents from {data_dir}")
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        raw_docs = []
        for filepath in data_dir.glob("**/*.md"):
            try:
                loader = TextLoader(str(filepath), encoding='utf-8')
                raw_docs.extend(loader.load())
            except Exception as e:
                logger.warning(f"Failed to load {filepath}: {e}")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_documents(raw_docs)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if sample_ratio < 1.0:
            import random
            sample_size = int(len(chunks) * sample_ratio)
            chunks = random.sample(chunks, sample_size)
            logger.info(f"ğŸ“Š Sampled {len(chunks)} chunks ({sample_ratio*100:.0f}%)")
        
        # ColBERTDocumentä½œæˆ
        self.documents = [
            ColBERTDocument(
                content=chunk.page_content[:max_length],
                doc_id=i,
                metadata=chunk.metadata
            )
            for i, chunk in enumerate(chunks)
        ]
        
        logger.info(f"âœ“ Loaded {len(self.documents)} documents")
    
    def encode_documents_batch(self, batch_size: int = 16):
        """
        æ–‡æ›¸ã‚’ãƒãƒƒãƒã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«åŸ‹ã‚è¾¼ã¿ï¼‰
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        logger.info(f"Encoding {len(self.documents)} documents...")
        
        for i in range(0, len(self.documents), batch_size):
            batch = self.documents[i:i + batch_size]
            texts = [doc.content for doc in batch]
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            ).to(self.device)
            
            # åŸ‹ã‚è¾¼ã¿å–å¾—
            with torch.no_grad():
                outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden_dim)
            
            # L2æ­£è¦åŒ–
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=2)
            
            # CPUä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            for j, doc in enumerate(batch):
                doc.embeddings = embeddings[j].cpu()
            
            # GPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if self.device == 'cuda':
                torch.cuda.empty_cache()
            
            if (i // batch_size) % 10 == 0:
                logger.info(f"  Encoded {min(i + batch_size, len(self.documents))}/{len(self.documents)} documents")
        
        logger.info("âœ“ Document encoding complete")
    
    def build_index(self, batch_size: int = 16):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        self.encode_documents_batch(batch_size=batch_size)
        self.index_built = True
        logger.info(f"âœ“ Index built with {len(self.documents)} documents")
    
    def compute_colbert_score(self, query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor) -> float:
        """
        ColBERT MaxSim ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            query_embeddings: ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ (query_len, hidden_dim)
            doc_embeddings: æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ (doc_len, hidden_dim)
        
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸMaxSimã‚¹ã‚³ã‚¢
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # GPUã«ç§»å‹•
        query_gpu = query_embeddings.to(device)
        doc_gpu = doc_embeddings.to(device)
        
        # æ­£è¦åŒ–
        query_norm = torch.nn.functional.normalize(query_gpu, p=2, dim=1)
        doc_norm = torch.nn.functional.normalize(doc_gpu, p=2, dim=1)
        
        # é¡ä¼¼åº¦è¡Œåˆ—: (query_len, doc_len)
        similarity_matrix = torch.matmul(query_norm, doc_norm.T)
        
        # MaxSim: å„ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æœ€å¤§é¡ä¼¼åº¦ã®åˆè¨ˆ
        max_similarities = similarity_matrix.max(dim=1)[0]
        
        # æ­£è¦åŒ–: ã‚¯ã‚¨ãƒªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§å‰²ã‚‹ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        num_query_tokens = query_embeddings.size(0)
        colbert_score = max_similarities.sum().item() / num_query_tokens
        
        return colbert_score
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """
        2æ®µéšæ¤œç´¢: mean pooling filter + ColBERT MaxSim
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ä»¶æ•°
        
        Returns:
            [(content, score, metadata), ...]
        """
        if not self.index_built:
            raise ValueError("Index not built. Call build_index() first.")
        
        # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        inputs = self.tokenizer(
            query,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            query_embeddings = outputs.last_hidden_state[0]  # (seq_len, hidden_dim)
        
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        # Stage 1: Mean Poolingé«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¸Šä½50å€™è£œï¼‰
        query_mean = query_embeddings.mean(dim=0).cpu()
        
        candidates = []
        for doc in self.documents:
            doc_mean = doc.embeddings.mean(dim=0)
            score = torch.dot(query_mean, doc_mean).item()
            candidates.append((doc, score))
        
        # ä¸Šä½50å€™è£œ
        candidates.sort(key=lambda x: x[1], reverse=True)
        top_candidates = candidates[:min(50, len(candidates))]
        
        # Stage 2: ColBERT MaxSimç²¾å¯†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        results = []
        for doc, _ in top_candidates:
            colbert_score = self.compute_colbert_score(query_embeddings, doc.embeddings)
            results.append((doc.content, colbert_score, doc.metadata))
        
        # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results[:top_k]
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "system_name": "ColBERT RAG",
            "model": self.model_name,
            "total_documents": len(self.documents),
            "index_built": self.index_built,
        }


def test_colbert_rag():
    """ColBERT RAGã®ãƒ†ã‚¹ãƒˆ"""
    from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, COLBERT_SAMPLE_RATIO
    import time
    
    print("\n=== ColBERT RAG Test ===")
    
    # åˆæœŸåŒ–
    rag = ColBERTRAG()
    
    # æ–‡æ›¸èª­ã¿è¾¼ã¿
    rag.load_documents(
        data_dir=DATA_DIR,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        sample_ratio=0.1  # ãƒ†ã‚¹ãƒˆç”¨10%
    )
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    rag.build_index(batch_size=16)
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "å ¤é˜²ã®å¤©ç«¯å¹…ã¯ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ",
        "ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆã®è¨­è¨ˆåŸºæº–å¼·åº¦ã¯ï¼Ÿ",
    ]
    
    for query in test_queries:
        print(f"\nQuery: {query}")
        start = time.time()
        results = rag.search(query, top_k=3)
        elapsed = (time.time() - start) * 1000
        
        print(f"Time: {elapsed:.2f}ms")
        for i, (content, score, metadata) in enumerate(results, 1):
            print(f"  {i}. Score: {score:.4f}")
            print(f"     {content[:100]}...")
    
    # çµ±è¨ˆ
    stats = rag.get_stats()
    print(f"\n{stats}")


if __name__ == "__main__":
    test_colbert_rag()
